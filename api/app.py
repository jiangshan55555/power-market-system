from flask import Flask, request, jsonify, send_from_directory, Response
from flask_cors import CORS
import pandas as pd
import numpy as np
from datetime import datetime
import os
import json
import sys
from pathlib import Path
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, VotingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# è‡ªå®šä¹‰ JSON ç¼–ç å™¨ï¼Œå¤„ç† NaN å’Œ Infinity
class SafeJSONEncoder(json.JSONEncoder):
    def encode(self, o):
        if isinstance(o, float):
            if np.isnan(o) or np.isinf(o):
                return 'null'
        return super().encode(o)

    def iterencode(self, o, _one_shot=False):
        """Encode the given object and yield each string representation as available."""
        for chunk in super().iterencode(o, _one_shot):
            # Replace NaN and Infinity with null in the JSON string
            chunk = chunk.replace('NaN', 'null').replace('Infinity', 'null').replace('-Infinity', 'null')
            yield chunk

# å¯¼å…¥ç‰¹å¾å·¥ç¨‹æ¨¡å—
from feature_engineering import create_all_features
from predict_all_models import run_all_models
from run_original_prediction import run_original_prediction
from run_bidding_optimization import run_bidding_optimization

# æ·»åŠ åŸæ¥é¡¹ç›®çš„è·¯å¾„ä»¥å¯¼å…¥æ¨¡å‹ç±»
ORIGINAL_PROJECT_PATH = Path(__file__).parent.parent.parent / 'power-market-system' / 'åŸæ¥çš„é¡¹ç›®èµ„æ–™'
sys.path.insert(0, str(ORIGINAL_PROJECT_PATH))

try:
    from src.predictions.random_forest_model import RandomForestModel
    from src.predictions.xgboost_model import XGBoostModel
    from src.predictions.gradient_boosting_model import GradientBoostingModel
    from src.predictions.ensemble_model import EnsembleModel
    USE_ORIGINAL_MODELS = True
    print("âœ… æˆåŠŸå¯¼å…¥åŸé¡¹ç›®çš„æ¨¡å‹ç±»")
    print("   - RandomForestModel")
    print("   - XGBoostModel")
    print("   - GradientBoostingModel")
    print("   - EnsembleModel")
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥åŸé¡¹ç›®æ¨¡å‹ç±»: {e}")
    print("å°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬çš„æ¨¡å‹")
    USE_ORIGINAL_MODELS = False

app = Flask(__name__)
CORS(app)

# é…ç½® Flask ä½¿ç”¨è‡ªå®šä¹‰ JSON ç¼–ç å™¨
app.json_encoder = SafeJSONEncoder
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False

UPLOAD_FOLDER = 'uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# å…¨å±€å˜é‡å­˜å‚¨æ•°æ®å’Œæ¨¡å‹
current_data = None
trained_models = None
scaler = None
feature_columns = None

@app.route('/')
def index():
    """æä¾›å‰ç«¯é¡µé¢"""
    return send_from_directory('..', 'index.html')

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'message': 'ç”µåŠ›å¸‚åœºé¢„æµ‹ç³»ç»Ÿè¿è¡Œæ­£å¸¸',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/upload', methods=['POST'])
def upload_file():
    global current_data

    try:
        if 'file' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰æ–‡ä»¶ä¸Šä¼ '}), 400

        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'æ–‡ä»¶åä¸ºç©º'}), 400

        # ä¿å­˜æ–‡ä»¶
        filepath = os.path.join(UPLOAD_FOLDER, file.filename)
        file.save(filepath)

        print(f"âœ… æ–‡ä»¶å·²ä¿å­˜åˆ°: {filepath}")

        # è¯»å–æ•°æ®ï¼ˆä½¿ç”¨åŒå±‚è¡¨å¤´ï¼‰
        if file.filename.endswith('.xlsx') or file.filename.endswith('.xls'):
            # å°è¯•è¯»å–åŒå±‚è¡¨å¤´
            try:
                current_data = pd.read_excel(filepath, header=[0, 1])
                print(f"âœ… ä½¿ç”¨åŒå±‚è¡¨å¤´è¯»å–æ•°æ®")

                # å¤„ç†å¤šå±‚è¡¨å¤´ï¼Œåˆå¹¶ä¸ºå•å±‚
                new_columns = []
                for col in current_data.columns.values:
                    feature_name = str(col[0]).strip()
                    # å¯¹äºæ—¶é—´åˆ—ï¼Œåªä¿ç•™ç¬¬ä¸€å±‚çš„åˆ—å
                    if feature_name in ['æ—¶é—´', 'æ—¥æœŸ', 'æ—¶åˆ»']:
                        new_columns.append(feature_name)
                    else:
                        # å¯¹äºå…¶ä»–åˆ—ï¼Œå¦‚æœç¬¬äºŒå±‚æœ‰å€¼ä¸”ä¸æ˜¯æ•°å­—ï¼Œåˆ™åˆå¹¶
                        if pd.notna(col[1]) and not isinstance(col[1], (int, float)):
                            second_part = str(col[1]).strip()
                            if second_part and second_part != feature_name:
                                feature_name = f"{feature_name}_{second_part}"
                        new_columns.append(feature_name)
                current_data.columns = new_columns
                print(f"   åˆå¹¶åçš„åˆ—å: {current_data.columns.tolist()[:10]}...")
            except:
                # å¦‚æœåŒå±‚è¡¨å¤´å¤±è´¥ï¼Œå°è¯•å•å±‚è¡¨å¤´
                print(f"   åŒå±‚è¡¨å¤´è¯»å–å¤±è´¥ï¼Œå°è¯•å•å±‚è¡¨å¤´")
                current_data = pd.read_excel(filepath)
        elif file.filename.endswith('.csv'):
            current_data = pd.read_csv(filepath)
        else:
            return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼  Excel æˆ– CSV æ–‡ä»¶'}), 400

        print(f"âœ… æ•°æ®è¯»å–æˆåŠŸï¼Œå½¢çŠ¶: {current_data.shape}")
        print(f"   å‰10åˆ—: {current_data.columns.tolist()[:10]}")

        # è‡ªåŠ¨è¯†åˆ«å¹¶è§£ææ—¶é—´åˆ—
        time_column = None
        for col in current_data.columns:
            col_lower = str(col).lower()
            # ä¼˜å…ˆæŸ¥æ‰¾"æ—¶é—´"åˆ—ï¼ˆè€Œä¸æ˜¯"æ—¥æœŸ"æˆ–"æ—¶åˆ»"ï¼‰
            if col_lower == 'æ—¶é—´' or col_lower == 'datetime':
                time_column = col
                break

        # å¦‚æœæ²¡æ‰¾åˆ°"æ—¶é—´"åˆ—ï¼Œå†æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ—¶é—´åˆ—ï¼ˆä½†æ’é™¤"æ—¥æœŸ"åˆ—ï¼‰
        if not time_column:
            for col in current_data.columns:
                col_lower = str(col).lower()
                # æ’é™¤"æ—¥æœŸ"å’Œ"æ—¶åˆ»"åˆ—ï¼ŒåªæŸ¥æ‰¾åŒ…å« time æˆ– date çš„åˆ—
                if any(keyword in col_lower for keyword in ['time', 'date']) and 'æ—¥æœŸ' not in col_lower and 'æ—¶åˆ»' not in col_lower:
                    time_column = col
                    break

        if time_column:
            print(f"   æ‰¾åˆ°æ—¶é—´åˆ—: {time_column}")
            print(f"   æ—¶é—´åˆ—æ•°æ®ç±»å‹: {current_data[time_column].dtype}")
            print(f"   æ—¶é—´åˆ—ç¤ºä¾‹å€¼: {current_data[time_column].head(3).tolist()}")

            # è§£ææ—¶é—´åˆ—
            try:
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œå°è¯•è§£æ
                if current_data[time_column].dtype == 'object':
                    # å…ˆä¿®å¤24:00çš„é—®é¢˜ï¼ˆ24:00åº”è¯¥æ˜¯æ¬¡æ—¥00:00ï¼‰
                    def fix_24_hour(time_str):
                        if pd.isna(time_str):
                            return time_str
                        time_str = str(time_str).strip()
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«24:00
                        if ' 24:00' in time_str:
                            # å°†24:00æ›¿æ¢ä¸º00:00ï¼Œå¹¶å°†æ—¥æœŸåŠ 1å¤©
                            date_part, time_part = time_str.split(' ')
                            date_obj = pd.to_datetime(date_part)
                            next_day = date_obj + pd.Timedelta(days=1)
                            return next_day.strftime('%Y-%m-%d') + ' 00:00'
                        return time_str

                    print(f"   ä¿®å¤24:00æ—¶é—´æ ¼å¼...")
                    current_data[time_column] = current_data[time_column].apply(fix_24_hour)
                    current_data[time_column] = pd.to_datetime(current_data[time_column], format='%Y-%m-%d %H:%M', errors='coerce')
                elif not pd.api.types.is_datetime64_any_dtype(current_data[time_column]):
                    current_data[time_column] = pd.to_datetime(current_data[time_column], errors='coerce')

                print(f"   âœ… æ—¶é—´åˆ—å·²è§£æä¸º datetime ç±»å‹")
                print(f"   æ—¶é—´èŒƒå›´: {current_data[time_column].min()} åˆ° {current_data[time_column].max()}")
            except Exception as e:
                print(f"   âš ï¸ æ—¶é—´åˆ—è§£æå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        else:
            print(f"   âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—")

        # è½¬æ¢æ•°æ®é¢„è§ˆä¸º JSON å¯åºåˆ—åŒ–æ ¼å¼
        preview_data = current_data.head(20).copy()

        # å°†æ‰€æœ‰ numpy/pandas ç±»å‹è½¬æ¢ä¸º Python åŸç”Ÿç±»å‹
        preview_dict = []
        for _, row in preview_data.iterrows():
            row_dict = {}
            for col in preview_data.columns:
                val = row[col]
                # è½¬æ¢ä¸º Python åŸç”Ÿç±»å‹
                if pd.isna(val):
                    row_dict[col] = None
                elif isinstance(val, (np.integer, np.int64, np.int32)):
                    row_dict[col] = int(val)
                elif isinstance(val, (np.floating, np.float64, np.float32)):
                    row_dict[col] = float(val)
                elif isinstance(val, (pd.Timestamp, np.datetime64)):
                    row_dict[col] = str(val)
                else:
                    row_dict[col] = str(val)
            preview_dict.append(row_dict)

        # æ•°æ®åŸºæœ¬ä¿¡æ¯
        info = {
            'filename': file.filename,
            'rows': int(len(current_data)),
            'columns': int(len(current_data.columns)),
            'column_names': [str(col) for col in current_data.columns.tolist()],
            'preview': preview_dict,
            'upload_time': datetime.now().isoformat()
        }

        print("æ•°æ®ä¿¡æ¯å‡†å¤‡å®Œæˆï¼Œå‡†å¤‡è¿”å›...")

        return jsonify({
            'message': 'æ–‡ä»¶ä¸Šä¼ æˆåŠŸ',
            'data': info
        })

    except Exception as e:
        import traceback
        error_msg = f'ä¸Šä¼ å¤±è´¥: {str(e)}'
        print(error_msg)
        print(traceback.format_exc())
        return jsonify({'error': error_msg}), 500

@app.route('/api/available-dates', methods=['GET'])
def get_available_dates():
    """è·å–æ•°æ®ä¸­æ‰€æœ‰å¯ç”¨çš„æ—¥æœŸåˆ—è¡¨"""
    global current_data

    try:
        if current_data is None:
            return jsonify({'error': 'è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶'}), 400

        # æ‰¾åˆ°æ—¶é—´åˆ—ï¼ˆä¼˜å…ˆæŸ¥æ‰¾"æ—¶é—´"åˆ—ï¼‰
        time_column = None
        for col in current_data.columns:
            col_lower = str(col).lower()
            if col_lower == 'æ—¶é—´' or col_lower == 'datetime':
                time_column = col
                break

        # å¦‚æœæ²¡æ‰¾åˆ°"æ—¶é—´"åˆ—ï¼Œå†æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ—¶é—´åˆ—
        if not time_column:
            for col in current_data.columns:
                col_lower = str(col).lower()
                if any(keyword in col_lower for keyword in ['time', 'date']):
                    time_column = col
                    break

        if time_column is None:
            return jsonify({'error': 'æ•°æ®ä¸­æœªæ‰¾åˆ°æ—¶é—´åˆ—'}), 400

        print(f"ğŸ“… è·å–å¯ç”¨æ—¥æœŸåˆ—è¡¨...")
        print(f"   ä½¿ç”¨æ—¶é—´åˆ—: {time_column}")
        print(f"   æ—¶é—´åˆ—æ•°æ®ç±»å‹: {current_data[time_column].dtype}")
        print(f"   æ—¶é—´åˆ—ç¤ºä¾‹å€¼: {current_data[time_column].head(3).tolist()}")

        # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
        if not pd.api.types.is_datetime64_any_dtype(current_data[time_column]):
            print(f"   âš ï¸ æ—¶é—´åˆ—ä¸æ˜¯ datetime ç±»å‹ï¼Œå°è¯•è½¬æ¢...")
            try:
                # å°è¯•ä½¿ç”¨æŒ‡å®šæ ¼å¼è§£æ
                current_data[time_column] = pd.to_datetime(current_data[time_column], format='%Y-%m-%d %H:%M', errors='coerce')
                print(f"   âœ… æ—¶é—´åˆ—å·²è½¬æ¢ä¸º datetime ç±»å‹")
            except Exception as e:
                print(f"   âŒ æ—¶é—´åˆ—è½¬æ¢å¤±è´¥: {e}")
                return jsonify({'error': f'æ—¶é—´åˆ—è½¬æ¢å¤±è´¥: {str(e)}'}), 400

        # è·å–æ‰€æœ‰å”¯ä¸€çš„æ—¥æœŸï¼ˆæ’é™¤ NaTï¼‰
        valid_times = current_data[time_column].dropna()
        unique_dates = valid_times.dt.date.unique()
        unique_dates = sorted(unique_dates, reverse=True)  # é™åºæ’åˆ—ï¼Œæœ€æ–°çš„åœ¨å‰

        print(f"   âœ… æ‰¾åˆ° {len(unique_dates)} ä¸ªå”¯ä¸€æ—¥æœŸ")
        if len(unique_dates) > 0:
            print(f"   æ—¥æœŸèŒƒå›´: {unique_dates[-1]} åˆ° {unique_dates[0]}")

        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
        date_list = [str(date) for date in unique_dates]

        return jsonify({
            'dates': date_list,
            'count': len(date_list)
        })

    except Exception as e:
        import traceback
        error_msg = f'è·å–æ—¥æœŸåˆ—è¡¨å¤±è´¥: {str(e)}'
        print(error_msg)
        print(traceback.format_exc())
        return jsonify({'error': error_msg}), 500

@app.route('/api/query-price', methods=['POST'])
def query_price():
    """æŸ¥è¯¢å†å²ç”µä»·æ•°æ®"""
    global current_data

    try:
        print("\n" + "="*60)
        print("ğŸ“Š æŸ¥è¯¢å†å²ç”µä»·æ•°æ®...")

        if current_data is None:
            return jsonify({'error': 'è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶'}), 400

        # è·å–æŸ¥è¯¢æ—¥æœŸå’Œæ—¶åˆ»
        data = request.get_json()
        query_date = data.get('date')
        query_time = data.get('time')  # å¯é€‰å‚æ•°ï¼Œå¦‚æœæä¾›åˆ™æŸ¥è¯¢ç‰¹å®šæ—¶åˆ»

        print(f"   æ”¶åˆ°æŸ¥è¯¢è¯·æ±‚:")
        print(f"   - æ—¥æœŸ: {query_date} (ç±»å‹: {type(query_date)})")
        print(f"   - æ—¶åˆ»: {query_time}")

        if not query_date:
            return jsonify({'error': 'è¯·æä¾›æŸ¥è¯¢æ—¥æœŸ'}), 400

        # ç¡®ä¿æ•°æ®ä¸­æœ‰æ—¶é—´åˆ—
        # ä¼˜å…ˆæŸ¥æ‰¾ "æ—¶é—´" åˆ—ï¼ˆåŒ…å«å®Œæ•´çš„æ—¥æœŸå’Œæ—¶é—´ä¿¡æ¯ï¼‰
        time_column = None

        # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šæŸ¥æ‰¾ "æ—¶é—´" åˆ—
        for col in current_data.columns:
            if str(col).lower() == 'æ—¶é—´' or str(col).lower() == 'datetime':
                time_column = col
                print(f"   âœ… æ‰¾åˆ°æ—¶é—´åˆ—: {time_column}")
                break

        # ç¬¬äºŒä¼˜å…ˆçº§ï¼šå¦‚æœæ²¡æ‰¾åˆ° "æ—¶é—´" åˆ—ï¼Œå†æŸ¥æ‰¾å…¶ä»–åŒ…å« time æˆ– date çš„åˆ—ï¼ˆä½†æ’é™¤ "æ—¥æœŸ" å’Œ "æ—¶åˆ»"ï¼‰
        if not time_column:
            for col in current_data.columns:
                col_lower = str(col).lower()
                if any(keyword in col_lower for keyword in ['time', 'date']) and 'æ—¥æœŸ' not in col_lower and 'æ—¶åˆ»' not in col_lower:
                    time_column = col
                    print(f"   âœ… æ‰¾åˆ°æ—¶é—´åˆ—: {time_column}")
                    break

        if time_column is None:
            print(f"   âŒ æœªæ‰¾åˆ°æ—¶é—´åˆ—ï¼Œå¯ç”¨åˆ—: {current_data.columns.tolist()}")
            return jsonify({'error': 'æ•°æ®ä¸­æœªæ‰¾åˆ°æ—¶é—´åˆ—'}), 400

        # æ‰“å°æ—¶é—´åˆ—çš„å½“å‰çŠ¶æ€ï¼ˆåœ¨ä»»ä½•è½¬æ¢ä¹‹å‰ï¼‰
        print(f"   æ—¶é—´åˆ— '{time_column}' å½“å‰ç±»å‹: {current_data[time_column].dtype}")
        print(f"   æ—¶é—´åˆ—å‰3ä¸ªå€¼: {current_data[time_column].head(3).tolist()}")

        # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹ï¼ˆå¦‚æœå·²ç»æ˜¯ datetime ç±»å‹ï¼Œå°±ä¸è¦å†è½¬æ¢äº†ï¼‰
        if not pd.api.types.is_datetime64_any_dtype(current_data[time_column]):
            print(f"   âš ï¸  æ—¶é—´åˆ—ä¸æ˜¯ datetime ç±»å‹ï¼Œæ­£åœ¨è½¬æ¢...")
            current_data[time_column] = pd.to_datetime(current_data[time_column])
            print(f"   âœ… æ—¶é—´åˆ—å·²è½¬æ¢ä¸º datetime ç±»å‹")
            print(f"   è½¬æ¢åå‰3ä¸ªå€¼: {current_data[time_column].head(3).tolist()}")
        else:
            print(f"   âœ… æ—¶é—´åˆ—å·²ç»æ˜¯ datetime ç±»å‹ï¼Œæ— éœ€è½¬æ¢")

        # æ‰¾åˆ°ç”µä»·åˆ—ï¼ˆä¼˜å…ˆä½¿ç”¨"å®æ—¶å‡ºæ¸…ç”µä»·"ï¼‰
        price_column = None

        # ä¼˜å…ˆæŸ¥æ‰¾"å®æ—¶å‡ºæ¸…ç”µä»·"
        if 'å®æ—¶å‡ºæ¸…ç”µä»·' in current_data.columns:
            price_column = 'å®æ—¶å‡ºæ¸…ç”µä»·'
            print(f"   âœ… æ‰¾åˆ°ç”µä»·åˆ—: {price_column}")
        else:
            # å¦‚æœæ²¡æœ‰ï¼Œå†æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„ç”µä»·åˆ—
            for col in current_data.columns:
                col_lower = str(col).lower()
                if any(keyword in col_lower for keyword in ['ç”µä»·', 'price', 'ä»·æ ¼', 'å‡ºæ¸…ä»·']):
                    price_column = col
                    print(f"   âœ… æ‰¾åˆ°ç”µä»·åˆ—: {price_column}")
                    break

        if price_column is None:
            print(f"   âŒ æœªæ‰¾åˆ°ç”µä»·åˆ—ï¼Œå¯ç”¨åˆ—: {current_data.columns.tolist()}")
            return jsonify({'error': 'æ•°æ®ä¸­æœªæ‰¾åˆ°ç”µä»·åˆ—'}), 400

        # ç­›é€‰æŒ‡å®šæ—¥æœŸçš„æ•°æ®
        query_date_obj = pd.to_datetime(query_date)
        print(f"   ğŸ“… æŸ¥è¯¢æ—¥æœŸ: {query_date} -> {query_date_obj.date()}")
        print(f"   æ—¶é—´åˆ—æ•°æ®ç±»å‹: {current_data[time_column].dtype}")
        print(f"   æ—¶é—´åˆ—ç¤ºä¾‹: {current_data[time_column].head(3).tolist()}")

        filtered_data = current_data[current_data[time_column].dt.date == query_date_obj.date()].copy()

        print(f"   ç­›é€‰åæ•°æ®é‡: {len(filtered_data)}")

        if len(filtered_data) == 0:
            # æ˜¾ç¤ºå¯ç”¨çš„æ—¥æœŸèŒƒå›´
            available_dates = current_data[time_column].dt.date.unique()
            print(f"   âŒ æœªæ‰¾åˆ°æ•°æ®ï¼Œå¯ç”¨æ—¥æœŸ: {sorted(available_dates)[:5]}...")
            return jsonify({'error': f'æœªæ‰¾åˆ° {query_date} çš„æ•°æ®ï¼Œè¯·æ£€æŸ¥æ—¥æœŸæ ¼å¼'}), 404

        # å¦‚æœæä¾›äº†æ—¶åˆ»ï¼Œè¿›ä¸€æ­¥ç­›é€‰ç‰¹å®šæ—¶åˆ»çš„æ•°æ®
        if query_time:
            query_datetime = pd.to_datetime(query_time)
            filtered_data = filtered_data[filtered_data[time_column] == query_datetime].copy()

            if len(filtered_data) == 0:
                return jsonify({'error': f'æœªæ‰¾åˆ° {query_time} çš„æ•°æ®'}), 404

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        price_values = filtered_data[price_column].dropna()

        stats = {
            'count': int(len(price_values)),
            'max': float(price_values.max()),
            'min': float(price_values.min()),
            'mean': float(price_values.mean())
        }

        # è·å–è¯¦ç»†æ•°æ®ï¼ˆæ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ—¶é—´æ’åºï¼‰
        filtered_data = filtered_data.sort_values(by=time_column)
        detail_data = []
        for _, row in filtered_data.iterrows():
            detail_row = {
                'time': str(row[time_column]),
                'price': float(row[price_column]) if not pd.isna(row[price_column]) else None
            }

            # å¦‚æœæœ‰è´Ÿè·åˆ—ï¼Œä¹Ÿæ·»åŠ è¿›å»
            for col in current_data.columns:
                col_lower = str(col).lower()
                if any(keyword in col_lower for keyword in ['è´Ÿè·', 'load', 'åŠŸç‡', 'power']):
                    detail_row['load'] = float(row[col]) if not pd.isna(row[col]) else None
                    break

            detail_data.append(detail_row)

        return jsonify({
            'date': query_date,
            'stats': stats,
            'detail': detail_data,
            'columns': {
                'time': time_column,
                'price': price_column
            }
        })

    except Exception as e:
        import traceback
        error_msg = f'æŸ¥è¯¢å¤±è´¥: {str(e)}'
        print(error_msg)
        print(traceback.format_exc())
        return jsonify({'error': error_msg}), 500

@app.route('/api/data-status', methods=['GET'])
def get_data_status():
    """è·å–æ•°æ®çŠ¶æ€è¯¦ç»†ä¿¡æ¯"""
    global current_data

    try:
        if current_data is None:
            return jsonify({'error': 'è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶'}), 400

        # åŸºæœ¬ä¿¡æ¯
        basic_info = {
            'rows': int(len(current_data)),
            'columns': int(len(current_data.columns)),
            'memory_usage': f"{current_data.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB"
        }

        # åˆ—ä¿¡æ¯ï¼ˆæ•°æ®ç±»å‹ã€ç¼ºå¤±å€¼ã€å”¯ä¸€å€¼ï¼‰
        column_info = []
        for col in current_data.columns:
            col_data = current_data[col]
            missing_count = int(col_data.isna().sum())
            missing_percent = float(missing_count / len(current_data) * 100)

            col_info = {
                'name': str(col),
                'dtype': str(col_data.dtype),
                'missing_count': missing_count,
                'missing_percent': round(missing_percent, 2),
                'unique_count': int(col_data.nunique()),
                'non_null_count': int(col_data.count())
            }

            # å¦‚æœæ˜¯æ•°å€¼ç±»å‹ï¼Œæ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            if pd.api.types.is_numeric_dtype(col_data):
                stats = col_data.describe()
                col_info['statistics'] = {
                    'mean': float(stats['mean']) if not pd.isna(stats['mean']) else None,
                    'std': float(stats['std']) if not pd.isna(stats['std']) else None,
                    'min': float(stats['min']) if not pd.isna(stats['min']) else None,
                    'max': float(stats['max']) if not pd.isna(stats['max']) else None,
                    'median': float(col_data.median()) if not pd.isna(col_data.median()) else None
                }

            column_info.append(col_info)

        # æ•°æ®è´¨é‡è¯„åˆ†
        total_cells = len(current_data) * len(current_data.columns)
        missing_cells = current_data.isna().sum().sum()
        data_quality_score = float((total_cells - missing_cells) / total_cells * 100)

        # æ•°æ®é¢„è§ˆï¼ˆå‰20è¡Œï¼‰
        preview_data = current_data.head(20).copy()
        preview_dict = []
        for _, row in preview_data.iterrows():
            row_dict = {}
            for col in preview_data.columns:
                val = row[col]
                if pd.isna(val):
                    row_dict[col] = None
                elif isinstance(val, (np.integer, np.int64, np.int32)):
                    row_dict[col] = int(val)
                elif isinstance(val, (np.floating, np.float64, np.float32)):
                    row_dict[col] = float(val)
                elif isinstance(val, (pd.Timestamp, np.datetime64)):
                    row_dict[col] = str(val)
                else:
                    row_dict[col] = str(val)
            preview_dict.append(row_dict)

        result = {
            'basic_info': basic_info,
            'column_info': column_info,
            'data_quality_score': round(data_quality_score, 2),
            'preview': preview_dict
        }

        print(f"âœ… æ•°æ®çŠ¶æ€æ£€æŸ¥å®Œæˆ")
        print(f"   æ•°æ®è´¨é‡è¯„åˆ†: {data_quality_score:.2f}%")
        print(f"   æ€»ç¼ºå¤±å€¼: {int(missing_cells)} / {total_cells}")

        return jsonify(result)

    except Exception as e:
        import traceback
        error_msg = f'è·å–æ•°æ®çŠ¶æ€å¤±è´¥: {str(e)}'
        print(error_msg)
        print(traceback.format_exc())
        return jsonify({'error': error_msg}), 500

@app.route('/api/train', methods=['POST'])
def train_model():
    global current_data, trained_models, scaler, feature_columns

    try:
        if current_data is None:
            return jsonify({'error': 'è¯·å…ˆä¸Šä¼ æ•°æ®'}), 400

        params = request.json
        target_column = params.get('target_column', 'ç”µä»·')

        if target_column not in current_data.columns:
            return jsonify({'error': f'ç›®æ ‡åˆ— "{target_column}" ä¸å­˜åœ¨'}), 400

        # å‡†å¤‡æ•°æ®
        df = current_data.copy()

        # é€‰æ‹©æ•°å€¼å‹ç‰¹å¾
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        if target_column not in numeric_columns:
            return jsonify({'error': f'ç›®æ ‡åˆ— "{target_column}" ä¸æ˜¯æ•°å€¼å‹'}), 400

        feature_columns = [col for col in numeric_columns if col != target_column]

        if len(feature_columns) == 0:
            return jsonify({'error': 'æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾åˆ—'}), 400

        X = df[feature_columns].fillna(df[feature_columns].mean())
        y = df[target_column].fillna(df[target_column].mean())

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        print("=" * 60)
        print("å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨åŸé¡¹ç›®çš„æ¨¡å‹å®ç°ï¼‰...")
        print("=" * 60)

        trained_models = {}

        # 1. Random Forest æ¨¡å‹ï¼ˆä½¿ç”¨åŸé¡¹ç›®çš„å®ç°ï¼‰
        print("è®­ç»ƒ Random Forest æ¨¡å‹...")
        if USE_ORIGINAL_MODELS:
            try:
                rf_model = RandomForestModel()
                if rf_model.train(X_train_scaled, y_train):
                    trained_models['random_forest'] = rf_model
                    print("âœ… Random Forest æ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆä½¿ç”¨åŸé¡¹ç›®å®ç°ï¼‰")
                else:
                    raise Exception("åŸé¡¹ç›®æ¨¡å‹è®­ç»ƒå¤±è´¥")
            except Exception as e:
                print(f"âš ï¸ åŸé¡¹ç›® RF æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬: {e}")
                import traceback
                traceback.print_exc()
                rf_model = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)
                rf_model.fit(X_train_scaled, y_train)
                trained_models['random_forest'] = rf_model
                print("âœ… Random Forest æ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰")
        else:
            rf_model = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)
            rf_model.fit(X_train_scaled, y_train)
            trained_models['random_forest'] = rf_model
            print("âœ… Random Forest æ¨¡å‹è®­ç»ƒå®Œæˆ")

        # 2. XGBoost æ¨¡å‹ï¼ˆä½¿ç”¨åŸé¡¹ç›®çš„å®ç°ï¼‰
        print("è®­ç»ƒ XGBoost æ¨¡å‹...")
        if USE_ORIGINAL_MODELS:
            try:
                xgb_model = XGBoostModel()
                if xgb_model.train(X_train_scaled, y_train):
                    trained_models['xgboost'] = xgb_model
                    print("âœ… XGBoost æ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆä½¿ç”¨åŸé¡¹ç›®å®ç°ï¼‰")
                else:
                    raise Exception("åŸé¡¹ç›®æ¨¡å‹è®­ç»ƒå¤±è´¥")
            except Exception as e:
                print(f"âš ï¸ åŸé¡¹ç›® XGBoost æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬: {e}")
                import traceback
                traceback.print_exc()
                xgb_model = XGBRegressor(n_estimators=100, random_state=42, max_depth=6, learning_rate=0.1)
                xgb_model.fit(X_train_scaled, y_train)
                trained_models['xgboost'] = xgb_model
                print("âœ… XGBoost æ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰")
        else:
            xgb_model = XGBRegressor(n_estimators=100, random_state=42, max_depth=6, learning_rate=0.1)
            xgb_model.fit(X_train_scaled, y_train)
            trained_models['xgboost'] = xgb_model
            print("âœ… XGBoost æ¨¡å‹è®­ç»ƒå®Œæˆ")

        # 3. Gradient Boosting æ¨¡å‹ï¼ˆä½¿ç”¨åŸé¡¹ç›®çš„å®ç°ï¼‰
        print("è®­ç»ƒ Gradient Boosting æ¨¡å‹...")
        if USE_ORIGINAL_MODELS:
            try:
                gb_model = GradientBoostingModel()
                if gb_model.train(X_train_scaled, y_train, hyperparameter_tuning=False):
                    trained_models['gradient_boosting'] = gb_model
                    print("âœ… Gradient Boosting æ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆä½¿ç”¨åŸé¡¹ç›®å®ç°ï¼‰")
                else:
                    raise Exception("åŸé¡¹ç›®æ¨¡å‹è®­ç»ƒå¤±è´¥")
            except Exception as e:
                print(f"âš ï¸ åŸé¡¹ç›® GB æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬: {e}")
                import traceback
                traceback.print_exc()
                from sklearn.ensemble import GradientBoostingRegressor
                gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=5)
                gb_model.fit(X_train_scaled, y_train)
                trained_models['gradient_boosting'] = gb_model
                print("âœ… Gradient Boosting æ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰")
        else:
            from sklearn.ensemble import GradientBoostingRegressor
            gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=5)
            gb_model.fit(X_train_scaled, y_train)
            trained_models['gradient_boosting'] = gb_model
            print("âœ… Gradient Boosting æ¨¡å‹è®­ç»ƒå®Œæˆ")

        # 4. é›†æˆæ¨¡å‹ï¼ˆä½¿ç”¨åŸé¡¹ç›®çš„æ™ºèƒ½é›†æˆï¼‰
        print("åˆ›å»ºé›†æˆæ¨¡å‹...")
        predictions_for_ensemble = {}
        for name, model in trained_models.items():
            try:
                predictions_for_ensemble[name] = model.predict(X_test_scaled)
            except Exception as e:
                print(f"âš ï¸ è·å– {name} é¢„æµ‹å¤±è´¥: {e}")

        if USE_ORIGINAL_MODELS and len(predictions_for_ensemble) >= 2:
            try:
                ensemble_config = {
                    'selection_method': 'all',
                    'ensemble_method': 'weighted_average',
                    'exclude_models': [],
                    'min_models': 2,
                }
                ensemble_model = EnsembleModel(config=ensemble_config)
                ensemble_model.train(predictions_for_ensemble, y_test)
                trained_models['ensemble'] = ensemble_model
                print("âœ… é›†æˆæ¨¡å‹åˆ›å»ºå®Œæˆï¼ˆä½¿ç”¨åŸé¡¹ç›®æ™ºèƒ½é›†æˆï¼‰")
            except Exception as e:
                print(f"âš ï¸ åŸé¡¹ç›®é›†æˆæ¨¡å‹å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                print("è·³è¿‡é›†æˆæ¨¡å‹")

        print("=" * 60)

        # è¯„ä¼°æ¨¡å‹
        results = {}
        for name, model in trained_models.items():
            try:
                y_pred = model.predict(X_test_scaled)
                results[name] = {
                    'mse': float(mean_squared_error(y_test, y_pred)),
                    'rmse': float(np.sqrt(mean_squared_error(y_test, y_pred))),
                    'mae': float(mean_absolute_error(y_test, y_pred)),
                    'r2': float(r2_score(y_test, y_pred))
                }
                print(f"  {name}: MAE={results[name]['mae']:.2f}, RMSE={results[name]['rmse']:.2f}, RÂ²={results[name]['r2']:.4f}")
            except Exception as e:
                print(f"âš ï¸ è¯„ä¼° {name} æ¨¡å‹å¤±è´¥: {e}")

        print("=" * 60)

        return jsonify({
            'message': 'æ¨¡å‹è®­ç»ƒæˆåŠŸ',
            'target_column': target_column,
            'features': feature_columns,
            'train_size': len(X_train),
            'test_size': len(X_test),
            'results': results,
            'using_original_models': USE_ORIGINAL_MODELS
        })

    except Exception as e:
        import traceback
        print(f"è®­ç»ƒå¤±è´¥: {e}")
        print(traceback.format_exc())
        return jsonify({'error': f'è®­ç»ƒå¤±è´¥: {str(e)}'}), 500

@app.route('/api/batch_predict', methods=['POST'])
def batch_predict():
    global current_data, trained_models, scaler, feature_columns

    try:
        if trained_models is None:
            return jsonify({'error': 'è¯·å…ˆè®­ç»ƒæ¨¡å‹'}), 400

        if current_data is None:
            return jsonify({'error': 'è¯·å…ˆä¸Šä¼ æ•°æ®'}), 400

        params = request.json
        model_type = params.get('model', 'ensemble')

        if model_type not in trained_models:
            available_models = list(trained_models.keys())
            return jsonify({
                'error': f'æ¨¡å‹ç±»å‹ "{model_type}" ä¸å­˜åœ¨',
                'available_models': available_models
            }), 400

        # å‡†å¤‡æ•°æ®
        df = current_data.copy()
        X = df[feature_columns].fillna(df[feature_columns].mean())
        X_scaled = scaler.transform(X)

        # æ‰¹é‡é¢„æµ‹
        model = trained_models[model_type]
        print(f"ä½¿ç”¨æ¨¡å‹ {model_type} è¿›è¡Œæ‰¹é‡é¢„æµ‹...")
        print(f"æ¨¡å‹ç±»å‹: {type(model)}")

        # ç‰¹æ®Šå¤„ç†é›†æˆæ¨¡å‹
        if model_type == 'ensemble' and USE_ORIGINAL_MODELS:
            print("é›†æˆæ¨¡å‹éœ€è¦å„ä¸ªå­æ¨¡å‹çš„é¢„æµ‹ç»“æœ...")
            # è·å–æ‰€æœ‰å­æ¨¡å‹çš„é¢„æµ‹
            new_predictions = {}
            for name, sub_model in trained_models.items():
                if name != 'ensemble':
                    try:
                        new_predictions[name] = sub_model.predict(X_scaled)
                        print(f"  âœ… {name} é¢„æµ‹å®Œæˆ")
                    except Exception as e:
                        print(f"  âš ï¸ {name} é¢„æµ‹å¤±è´¥: {e}")

            if len(new_predictions) < 2:
                return jsonify({'error': 'é›†æˆæ¨¡å‹éœ€è¦è‡³å°‘2ä¸ªå­æ¨¡å‹çš„é¢„æµ‹ç»“æœ'}), 500

            # è°ƒç”¨é›†æˆæ¨¡å‹çš„ predict æ–¹æ³•
            predictions = model.predict(new_predictions)
            print(f"é›†æˆé¢„æµ‹å®Œæˆï¼Œç»“æœé•¿åº¦: {len(predictions) if predictions is not None else 0}")
        else:
            # æ™®é€šæ¨¡å‹ç›´æ¥é¢„æµ‹
            if hasattr(model, 'predict'):
                predictions = model.predict(X_scaled)
                print(f"é¢„æµ‹ç»“æœç±»å‹: {type(predictions)}")
                print(f"é¢„æµ‹ç»“æœå‰5ä¸ª: {predictions[:5] if predictions is not None else None}")
            else:
                return jsonify({'error': f'æ¨¡å‹ {model_type} æ²¡æœ‰ predict æ–¹æ³•'}), 500

        if predictions is None:
            return jsonify({'error': f'æ¨¡å‹ {model_type} é¢„æµ‹è¿”å› None'}), 500

        # æ·»åŠ é¢„æµ‹ç»“æœåˆ°æ•°æ®æ¡†
        df['é¢„æµ‹ç”µä»·'] = predictions

        # è½¬æ¢ä¸º JSON å¯åºåˆ—åŒ–çš„æ ¼å¼
        results = df.head(50).copy()
        for col in results.columns:
            if results[col].dtype == 'object':
                results[col] = results[col].astype(str)
            elif pd.api.types.is_datetime64_any_dtype(results[col]):
                results[col] = results[col].astype(str)
            else:
                results[col] = results[col].apply(lambda x: float(x) if pd.notna(x) else None)

        return jsonify({
            'message': 'æ‰¹é‡é¢„æµ‹å®Œæˆ',
            'model': model_type,
            'count': int(len(predictions)),
            'results': results.to_dict('records')
        })

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'æ‰¹é‡é¢„æµ‹å¤±è´¥: {str(e)}'}), 500

@app.route('/api/predict', methods=['POST'])
def predict_price():
    """é¢„æµ‹æ•´ä¸ªæœˆçš„ç”µä»·"""
    global current_data

    try:
        print("\n" + "="*60)
        print("ğŸ“¥ æ”¶åˆ°é¢„æµ‹è¯·æ±‚")
        print("="*60)

        if current_data is None:
            print("âŒ é”™è¯¯: æœªä¸Šä¼ æ•°æ®æ–‡ä»¶")
            return jsonify({'success': False, 'error': 'è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶'}), 400

        data = request.get_json()
        print(f"ğŸ“¦ è¯·æ±‚æ•°æ®: {data}")

        model_type = data.get('model', 'ensemble')
        print(f"ğŸ¤– æ¨¡å‹ç±»å‹: {model_type}")

        print(f"\n{'='*60}")
        print(f"ğŸ“Š å¼€å§‹é¢„æµ‹æ•´æœˆç”µä»· - æ¨¡å‹: {model_type}")
        print(f"{'='*60}")

        # æ‰¾åˆ°æ—¶é—´åˆ—
        time_column = None
        for col in current_data.columns:
            if str(col).lower() == 'æ—¶é—´' or str(col).lower() == 'datetime':
                time_column = col
                break

        if not time_column:
            for col in current_data.columns:
                col_lower = str(col).lower()
                if any(keyword in col_lower for keyword in ['time', 'date']) and 'æ—¥æœŸ' not in col_lower and 'æ—¶åˆ»' not in col_lower:
                    time_column = col
                    break

        if not time_column:
            error_msg = 'æœªæ‰¾åˆ°æ—¶é—´åˆ—'
            print(f"âŒ {error_msg}")
            return jsonify({'success': False, 'error': error_msg}), 400

        print(f"âœ… æ‰¾åˆ°æ—¶é—´åˆ—: {time_column}")

        # ç¡®ä¿æ—¶é—´åˆ—æ˜¯ datetime ç±»å‹
        if not pd.api.types.is_datetime64_any_dtype(current_data[time_column]):
            current_data[time_column] = pd.to_datetime(current_data[time_column])

        # æŒ‰æ—¶é—´æ’åº
        data_sorted = current_data.sort_values(by=time_column).copy()

        # è·å–æ—¥æœŸèŒƒå›´
        min_date = data_sorted[time_column].min().strftime('%Y-%m-%d')
        max_date = data_sorted[time_column].max().strftime('%Y-%m-%d')
        print(f"ğŸ“… æ•°æ®æ—¥æœŸèŒƒå›´: {min_date} åˆ° {max_date}")

        # æ‰¾åˆ°ç”µä»·åˆ—
        price_column = None
        for col in current_data.columns:
            if 'å®æ—¶å‡ºæ¸…ç”µä»·' in str(col):
                price_column = col
                break

        # å¦‚æœæ²¡æ‰¾åˆ°"å®æ—¶å‡ºæ¸…ç”µä»·"ï¼Œå†æ‰¾å…¶ä»–ç”µä»·åˆ—
        if not price_column:
            for col in current_data.columns:
                if 'ç”µä»·' in str(col):
                    price_column = col
                    break

        if not price_column:
            error_msg = f'æœªæ‰¾åˆ°ç”µä»·åˆ—ã€‚å¯ç”¨åˆ—: {list(current_data.columns)}'
            print(f"âŒ {error_msg}")
            return jsonify({'success': False, 'error': 'æœªæ‰¾åˆ°ç”µä»·åˆ—'}), 400

        print(f"âœ… ç”µä»·åˆ—: {price_column}")

        # ğŸ”§ ç‰¹å¾å·¥ç¨‹ï¼šåˆ›å»ºæ‰€æœ‰ç‰¹å¾ï¼ˆåŒ…æ‹¬GAPè§„åˆ™ï¼‰
        print(f"\n{'='*60}")
        print(f"ğŸ”§ å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        print(f"{'='*60}")

        # GAPå¤©æ•°è®¾ç½®ä¸º1ï¼ˆT=1ï¼‰
        gap_days = 1
        data_with_features = create_all_features(data_sorted, price_column, time_column, gap_days=gap_days)

        print(f"\n{'='*60}")
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ")
        print(f"{'='*60}\n")

        # æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²ï¼šå‰80%è®­ç»ƒï¼Œå20%æµ‹è¯•
        split_idx = int(len(data_with_features) * 0.8)
        train_data = data_with_features.iloc[:split_idx].copy().reset_index(drop=True)
        test_data = data_with_features.iloc[split_idx:].copy().reset_index(drop=True)

        print(f"âœ… è®­ç»ƒé›†å¤§å°: {len(train_data)}, æµ‹è¯•é›†å¤§å°: {len(test_data)}")
        print(f"   è®­ç»ƒé›†æ—¶é—´èŒƒå›´: {train_data[time_column].min()} åˆ° {train_data[time_column].max()}")
        print(f"   æµ‹è¯•é›†æ—¶é—´èŒƒå›´: {test_data[time_column].min()} åˆ° {test_data[time_column].max()}")

        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
        # åªä½¿ç”¨åŸé¡¹ç›®çš„5ä¸ªæ ¸å¿ƒç‰¹å¾
        feature_cols = ['hour', 'dayofweek', 'day', 'price_lag1', 'price_lag4']

        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
        missing_features = [f for f in feature_cols if f not in train_data.columns]
        if missing_features:
            error_msg = f'ç¼ºå°‘ç‰¹å¾åˆ—: {missing_features}'
            print(f"âŒ {error_msg}")
            return jsonify({'success': False, 'error': error_msg}), 400

        print(f"âœ… ä½¿ç”¨åŸé¡¹ç›®çš„5ä¸ªæ ¸å¿ƒç‰¹å¾:")
        print(f"   1. hour - å°æ—¶")
        print(f"   2. dayofweek - æ˜ŸæœŸå‡ ")
        print(f"   3. day - æ—¥æœŸ")
        print(f"   4. price_lag1 - å‰1ä¸ªæ—¶é—´ç‚¹çš„ä»·æ ¼")
        print(f"   5. price_lag4 - å‰4ä¸ªæ—¶é—´ç‚¹çš„ä»·æ ¼")

        # å¤„ç†ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨SimpleImputerï¼ŒåŸé¡¹ç›®æ–¹å¼ï¼‰
        print(f"\nğŸ”§ å¤„ç†ç¼ºå¤±å€¼...")
        print(f"   è®­ç»ƒé›†ç¼ºå¤±å€¼æ•°é‡: {train_data[feature_cols].isna().sum().sum()}")
        print(f"   æµ‹è¯•é›†ç¼ºå¤±å€¼æ•°é‡: {test_data[feature_cols].isna().sum().sum()}")

        X_train = train_data[feature_cols].values
        y_train = train_data[price_column].values
        X_test = test_data[feature_cols].values
        y_test = test_data[price_column].values

        # ä½¿ç”¨SimpleImputerå¤„ç†ç¼ºå¤±å€¼ï¼ˆåŸé¡¹ç›®æ–¹å¼ï¼‰
        from sklearn.impute import SimpleImputer
        imputer = SimpleImputer(strategy='mean')
        X_train = imputer.fit_transform(X_train)
        X_test = imputer.transform(X_test)

        print(f"   âœ… ç¼ºå¤±å€¼å¤„ç†å®Œæˆ")
        print(f"   âš ï¸  æ³¨æ„ï¼šä¸ä½¿ç”¨StandardScalerï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾å€¼ï¼ˆåŸé¡¹ç›®æ–¹å¼ï¼‰")

        # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹ç±»å‹è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹
        model_name_map = {
            'random_forest': 'éšæœºæ£®æ—',
            'xgboost': 'XGBoost',
            'gradient_boosting': 'æ¢¯åº¦æå‡',
            'linear_regression': 'çº¿æ€§å›å½’',
            'lstm': 'LSTMç¥ç»ç½‘ç»œ',
            'historical': 'å†å²åŒæœŸæ¨¡å‹',
            'ensemble': 'é›†æˆæ¨¡å‹'
        }

        print(f"ğŸ¤– å¼€å§‹è®­ç»ƒ {model_name_map.get(model_type, model_type)} æ¨¡å‹...")

        # å¯¼å…¥æ¨¡å‹
        sys.path.insert(0, str(Path(__file__).parent.parent))

        if model_type == 'random_forest':
            from src.predictions.random_forest_model import RandomForestModel
            model = RandomForestModel(config={'HYPERPARAMETER_TUNING': {'CV_FOLDS': 3, 'RF_SEARCH_ITERATIONS': 5}})
            model.train(X_train, y_train)
            y_pred = model.predict(X_test)

        elif model_type == 'xgboost':
            from src.predictions.xgboost_model import XGBoostModel
            model = XGBoostModel(config={'HYPERPARAMETER_TUNING': {'CV_FOLDS': 3, 'XGB_SEARCH_ITERATIONS': 5}})
            model.train(X_train, y_train)
            y_pred = model.predict(X_test)

        elif model_type == 'gradient_boosting':
            from src.predictions.gradient_boosting_model import GradientBoostingModel
            model = GradientBoostingModel(config={'HYPERPARAMETER_TUNING': {'CV_FOLDS': 3, 'GB_SEARCH_ITERATIONS': 5}})
            model.train(X_train, y_train)
            y_pred = model.predict(X_test)

        elif model_type == 'linear_regression':
            from src.predictions.linear_regression_model import LinearRegressionModel
            model = LinearRegressionModel(config={'HYPERPARAMETER_TUNING': {'CV_FOLDS': 3, 'LINEAR_SEARCH_ITERATIONS': 5}})
            model.train(X_train, y_train)
            y_pred = model.predict(X_test)

        elif model_type == 'lstm':
            from src.predictions.lstm_model import LSTMModel
            # LSTMéœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºå®ƒéœ€è¦åºåˆ—æ•°æ®
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨è¾ƒå°‘çš„epochs
            model = LSTMModel(config={
                'LSTM_PARAMS': {'epochs': 10, 'look_back_days': 3},
                'HYPERPARAMETER_TUNING': {'LSTM_SEARCH_ITERATIONS': 2}
            })
            model.train(X_train, y_train)
            y_pred = model.predict(X_test)

        elif model_type == 'historical':
            from src.predictions.historical_model import HistoricalModel
            # å†å²åŒæœŸæ¨¡å‹éœ€è¦å¸¦æ—¶é—´ç´¢å¼•çš„æ•°æ®
            # åˆ›å»ºå¸¦æ—¶é—´ç´¢å¼•çš„è®­ç»ƒæ•°æ®
            train_data_indexed = train_data.copy()
            train_data_indexed.index = pd.to_datetime(train_data[time_column])
            test_data_indexed = test_data.copy()
            test_data_indexed.index = pd.to_datetime(test_data[time_column])

            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X_train_df = train_data_indexed[feature_cols]
            y_train_series = train_data_indexed[price_column]
            y_train_series.name = price_column

            X_test_df = test_data_indexed[feature_cols]

            model = HistoricalModel()
            model.train(X_train_df, y_train_series)
            y_pred = model.predict(X_test_df)

        elif model_type == 'ensemble':
            from src.predictions.random_forest_model import RandomForestModel
            from src.predictions.xgboost_model import XGBoostModel
            from src.predictions.gradient_boosting_model import GradientBoostingModel
            from src.predictions.linear_regression_model import LinearRegressionModel
            from src.predictions.ensemble_model import EnsembleModel

            # è®­ç»ƒå„ä¸ªå­æ¨¡å‹
            print("   è®­ç»ƒéšæœºæ£®æ—...")
            rf_model = RandomForestModel(config={'HYPERPARAMETER_TUNING': {'CV_FOLDS': 3, 'RF_SEARCH_ITERATIONS': 3}})
            rf_model.train(X_train, y_train)
            rf_pred = rf_model.predict(X_test)

            print("   è®­ç»ƒXGBoost...")
            xgb_model = XGBoostModel(config={'HYPERPARAMETER_TUNING': {'CV_FOLDS': 3, 'XGB_SEARCH_ITERATIONS': 3}})
            xgb_model.train(X_train, y_train)
            xgb_pred = xgb_model.predict(X_test)

            print("   è®­ç»ƒæ¢¯åº¦æå‡...")
            gb_model = GradientBoostingModel(config={'HYPERPARAMETER_TUNING': {'CV_FOLDS': 3, 'GB_SEARCH_ITERATIONS': 3}})
            gb_model.train(X_train, y_train)
            gb_pred = gb_model.predict(X_test)

            print("   è®­ç»ƒçº¿æ€§å›å½’...")
            lr_model = LinearRegressionModel(config={'HYPERPARAMETER_TUNING': {'CV_FOLDS': 3, 'LINEAR_SEARCH_ITERATIONS': 3}})
            lr_model.train(X_train, y_train)
            lr_pred = lr_model.predict(X_test)

            # é›†æˆé¢„æµ‹
            print("   é›†æˆæ¨¡å‹...")
            ensemble = EnsembleModel(config={'ensemble_method': 'weighted_average', 'selection_method': 'all'})
            predictions_dict = {
                'random_forest': rf_pred,
                'xgboost': xgb_pred,
                'gradient_boosting': gb_pred,
                'linear_regression': lr_pred
            }
            ensemble.train(predictions_dict, y_test)
            y_pred = ensemble.predict(predictions_dict)

        else:
            return jsonify({'success': False, 'error': f'ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}'}), 400

        print(f"âœ… é¢„æµ‹å®Œæˆ")

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)

        # è®¡ç®—MAPEï¼Œå¤„ç†å¯èƒ½çš„NaNå€¼
        mape_values = np.abs((y_test - y_pred) / np.where(y_test != 0, y_test, 1)) * 100
        mape = np.mean(mape_values[~np.isnan(mape_values)])

        # å¦‚æœæ‰€æœ‰å€¼éƒ½æ˜¯NaNï¼Œè®¾ç½®ä¸º0
        if np.isnan(mape):
            mape = 0.0

        print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print(f"   MAE: {mae:.2f}")
        print(f"   RMSE: {rmse:.2f}")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   MAPE: {mape:.2f}%")

        # å‡†å¤‡è¿”å›ç»“æœ
        predictions = []
        # è·å–æ—¶é—´åˆ—çš„å€¼ï¼ˆä½œä¸ºåˆ—è¡¨ï¼‰
        time_values = test_data[time_column].tolist()

        print(f"ğŸ“Š å‡†å¤‡è¿”å›ç»“æœ...")
        print(f"   æµ‹è¯•é›†é•¿åº¦: {len(test_data)}")
        print(f"   é¢„æµ‹å€¼é•¿åº¦: {len(y_pred)}")
        print(f"   å®é™…å€¼é•¿åº¦: {len(y_test)}")
        print(f"   æ—¶é—´å€¼é•¿åº¦: {len(time_values)}")
        print(f"   å‰3ä¸ªæ—¶é—´å€¼: {time_values[:3]}")

        for i in range(len(test_data)):
            pred_val = float(y_pred[i]) if not np.isnan(y_pred[i]) and not np.isinf(y_pred[i]) else 0.0
            actual_val = float(y_test[i]) if not np.isnan(y_test[i]) and not np.isinf(y_test[i]) else 0.0
            time_val = time_values[i]

            # ç¡®ä¿æ—¶é—´å€¼æ˜¯æœ‰æ•ˆçš„
            if pd.isna(time_val):
                print(f"âš ï¸ è­¦å‘Š: ç´¢å¼• {i} çš„æ—¶é—´å€¼ä¸º NaT")
                time_str = 'NaT'
            else:
                time_str = str(time_val)

            predictions.append({
                'time': time_str,
                'predicted': pred_val,
                'actual': actual_val
            })

        # è·å–æµ‹è¯•é›†çš„æ—¥æœŸèŒƒå›´
        test_min_date = test_data[time_column].min().strftime('%Y-%m-%d')
        test_max_date = test_data[time_column].max().strftime('%Y-%m-%d')
        date_range = f"{test_min_date} åˆ° {test_max_date}"

        # ç¡®ä¿æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯æœ‰æ•ˆçš„æ•°å€¼
        def safe_float(val):
            if np.isnan(val) or np.isinf(val):
                return 0.0
            return float(val)

        return jsonify({
            'success': True,
            'date_range': date_range,
            'model': model_type,
            'model_name': model_name_map.get(model_type, model_type),
            'train_size': len(train_data),
            'predictions': predictions,
            'metrics': {
                'mae': safe_float(mae),
                'rmse': safe_float(rmse),
                'r2': safe_float(r2),
                'mape': safe_float(mape)
            }
        })

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'é¢„æµ‹å¤±è´¥: {str(e)}'}), 500

@app.route('/api/predict-original-file', methods=['POST'])
def predict_original_file_endpoint():
    """ç›´æ¥è°ƒç”¨åŸé¡¹ç›® main_prediction.py çš„ main() å‡½æ•°"""
    try:
        print("\n" + "="*60)
        print("ğŸ“¥ æ”¶åˆ°é¢„æµ‹è¯·æ±‚ - ç›´æ¥è°ƒç”¨åŸé¡¹ç›® main_prediction.py")
        print("="*60)

        # è°ƒç”¨åŸé¡¹ç›®çš„é¢„æµ‹å‡½æ•°ï¼ˆç›´æ¥è¿è¡Œmain()ï¼‰
        result = run_original_prediction()

        if not result['success']:
            return jsonify(result), 500

        # å‡†å¤‡è¿”å›æ•°æ®ï¼ˆæ ¼å¼åŒ–ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼ï¼‰
        predictions_list = []
        y_test = result['y_test']
        timestamps = result['timestamps']

        # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°
        def safe_float(value):
            """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œå¤„ç† None å€¼"""
            if value is None:
                return None
            try:
                return float(value)
            except (ValueError, TypeError):
                return None

        for i in range(len(y_test)):
            pred_item = {
                'time': timestamps[i],
                'actual': safe_float(y_test[i])
            }

            # æ·»åŠ æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹å€¼
            for model_name, pred_values in result['predictions'].items():
                if i < len(pred_values):
                    pred_item[model_name] = safe_float(pred_values[i])

            predictions_list.append(pred_item)

        print(f"\nâœ… åŸé¡¹ç›®é¢„æµ‹å®Œæˆï¼")
        print(f"   è¿”å› {len(predictions_list)} æ¡é¢„æµ‹ç»“æœ")
        print(f"   åŒ…å« {len(result['metrics'])} ä¸ªæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡")
        print(f"   æ—¶é—´æˆ³ç¤ºä¾‹: {predictions_list[0]['time'] if predictions_list else 'N/A'}")

        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            'success': True,
            'predictions': predictions_list,
            'metrics': result['metrics'],
            'model_names': list(result['predictions'].keys()),
            'train_size': result['train_size'],
            'test_size': result['test_size'],
            'feature_names': result['feature_names']
        }

        # æ‰‹åŠ¨åºåˆ—åŒ–ä¸º JSONï¼Œç¡®ä¿ NaN å’Œ Infinity è¢«æ­£ç¡®å¤„ç†
        json_str = json.dumps(response_data, cls=SafeJSONEncoder, ensure_ascii=False)

        # è¿”å› Response å¯¹è±¡è€Œä¸æ˜¯ jsonify
        return Response(json_str, mimetype='application/json')

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'é¢„æµ‹å¤±è´¥: {str(e)}'}), 500

@app.route('/api/predict-original', methods=['POST'])
def predict_original_endpoint():
    """è¿è¡ŒåŸé¡¹ç›®çš„é¢„æµ‹é€»è¾‘ï¼ˆä½¿ç”¨ä¸Šä¼ çš„æ•°æ®ï¼‰"""
    global current_data

    try:
        print("\n" + "="*60)
        print("ğŸ“¥ æ”¶åˆ°é¢„æµ‹è¯·æ±‚ - è¿è¡ŒåŸé¡¹ç›®é¢„æµ‹é€»è¾‘ï¼ˆä½¿ç”¨ä¸Šä¼ æ•°æ®ï¼‰")
        print("="*60)

        if current_data is None:
            print("âŒ é”™è¯¯: æœªä¸Šä¼ æ•°æ®æ–‡ä»¶")
            return jsonify({'success': False, 'error': 'è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶'}), 400

        # è·å–ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
        uploaded_file_path = os.path.join(UPLOAD_FOLDER, 'current_data.xlsx')

        # ä¿å­˜å½“å‰æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶
        current_data.to_excel(uploaded_file_path, index=False)
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {uploaded_file_path}")

        # è°ƒç”¨åŸé¡¹ç›®çš„é¢„æµ‹å‡½æ•°
        result = run_original_prediction(uploaded_file_path)

        if not result['success']:
            return jsonify(result), 500

        # å‡†å¤‡è¿”å›æ•°æ®ï¼ˆæ ¼å¼åŒ–ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼ï¼‰
        predictions_list = []
        y_test = result['y_test']
        timestamps = result['timestamps']

        for i in range(len(y_test)):
            pred_item = {
                'time': timestamps[i],
                'actual': float(y_test[i])
            }

            # æ·»åŠ æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹å€¼
            for model_name, pred_values in result['predictions'].items():
                if i < len(pred_values):
                    pred_item[model_name] = float(pred_values[i])

            predictions_list.append(pred_item)

        print(f"\nâœ… åŸé¡¹ç›®é¢„æµ‹å®Œæˆï¼")
        print(f"   è¿”å› {len(predictions_list)} æ¡é¢„æµ‹ç»“æœ")
        print(f"   åŒ…å« {len(result['metrics'])} ä¸ªæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡")

        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            'success': True,
            'predictions': predictions_list,
            'metrics': result['metrics'],
            'model_names': list(result['predictions'].keys()),
            'train_size': result['train_size'],
            'test_size': result['test_size'],
            'feature_names': result['feature_names']
        }

        # æ‰‹åŠ¨åºåˆ—åŒ–ä¸º JSONï¼Œç¡®ä¿ NaN å’Œ Infinity è¢«æ­£ç¡®å¤„ç†
        json_str = json.dumps(response_data, cls=SafeJSONEncoder, ensure_ascii=False)

        # è¿”å› Response å¯¹è±¡è€Œä¸æ˜¯ jsonify
        return Response(json_str, mimetype='application/json')

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'é¢„æµ‹å¤±è´¥: {str(e)}'}), 500

@app.route('/api/predict-all-models', methods=['POST'])
def predict_all_models_endpoint():
    """è¿è¡Œæ‰€æœ‰é¢„æµ‹æ¨¡å‹å¹¶è¿”å›ç»“æœ"""
    global current_data

    try:
        print("\n" + "="*60)
        print("ğŸ“¥ æ”¶åˆ°é¢„æµ‹è¯·æ±‚ - è¿è¡Œæ‰€æœ‰æ¨¡å‹")
        print("="*60)

        if current_data is None:
            print("âŒ é”™è¯¯: æœªä¸Šä¼ æ•°æ®æ–‡ä»¶")
            return jsonify({'success': False, 'error': 'è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶'}), 400

        # æ‰¾åˆ°æ—¶é—´åˆ—
        time_column = None
        for col in current_data.columns:
            if str(col).lower() == 'æ—¶é—´' or str(col).lower() == 'datetime':
                time_column = col
                break

        if not time_column:
            for col in current_data.columns:
                col_lower = str(col).lower()
                if any(keyword in col_lower for keyword in ['time', 'date']) and 'æ—¥æœŸ' not in col_lower and 'æ—¶åˆ»' not in col_lower:
                    time_column = col
                    break

        if not time_column:
            error_msg = 'æœªæ‰¾åˆ°æ—¶é—´åˆ—'
            print(f"âŒ {error_msg}")
            return jsonify({'success': False, 'error': error_msg}), 400

        print(f"âœ… æ‰¾åˆ°æ—¶é—´åˆ—: {time_column}")

        # ç¡®ä¿æ—¶é—´åˆ—æ˜¯ datetime ç±»å‹
        if not pd.api.types.is_datetime64_any_dtype(current_data[time_column]):
            current_data[time_column] = pd.to_datetime(current_data[time_column])

        # æŒ‰æ—¶é—´æ’åº
        data_sorted = current_data.sort_values(by=time_column).copy()

        # æ‰¾åˆ°ç”µä»·åˆ—
        price_column = None
        for col in current_data.columns:
            if 'å®æ—¶å‡ºæ¸…ç”µä»·' in str(col):
                price_column = col
                break

        if not price_column:
            for col in current_data.columns:
                if 'ç”µä»·' in str(col):
                    price_column = col
                    break

        if not price_column:
            error_msg = f'æœªæ‰¾åˆ°ç”µä»·åˆ—ã€‚å¯ç”¨åˆ—: {list(current_data.columns)}'
            print(f"âŒ {error_msg}")
            return jsonify({'success': False, 'error': 'æœªæ‰¾åˆ°ç”µä»·åˆ—'}), 400

        print(f"âœ… ç”µä»·åˆ—: {price_column}")

        # ç‰¹å¾å·¥ç¨‹
        gap_days = 1
        data_with_features = create_all_features(data_sorted, price_column, time_column, gap_days=gap_days)

        # åªä½¿ç”¨åŸé¡¹ç›®çš„5ä¸ªæ ¸å¿ƒç‰¹å¾
        feature_cols = ['hour', 'dayofweek', 'day', 'price_lag1', 'price_lag4']

        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
        missing_features = [f for f in feature_cols if f not in data_with_features.columns]
        if missing_features:
            error_msg = f'ç¼ºå°‘ç‰¹å¾åˆ—: {missing_features}'
            print(f"âŒ {error_msg}")
            return jsonify({'success': False, 'error': error_msg}), 400

        print(f"âœ… ä½¿ç”¨åŸé¡¹ç›®çš„5ä¸ªæ ¸å¿ƒç‰¹å¾:")
        print(f"   1. hour - å°æ—¶")
        print(f"   2. dayofweek - æ˜ŸæœŸå‡ ")
        print(f"   3. day - æ—¥æœŸ")
        print(f"   4. price_lag1 - å‰1ä¸ªæ—¶é—´ç‚¹çš„ä»·æ ¼")
        print(f"   5. price_lag4 - å‰4ä¸ªæ—¶é—´ç‚¹çš„ä»·æ ¼")

        # è¿è¡Œæ‰€æœ‰æ¨¡å‹
        results = run_all_models(data_with_features, price_column, time_column, feature_cols)

        # å‡†å¤‡è¿”å›æ•°æ®
        predictions_list = []
        time_values = results['timestamps'].tolist()
        y_test = results['y_test']

        for i in range(len(y_test)):
            pred_item = {
                'time': str(time_values[i]),
                'actual': float(y_test[i]) if not np.isnan(y_test[i]) else 0.0
            }

            # æ·»åŠ æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹å€¼
            for model_name, pred_values in results['predictions'].items():
                if i < len(pred_values):
                    pred_val = float(pred_values[i]) if not np.isnan(pred_values[i]) else 0.0
                    pred_item[model_name] = pred_val

            predictions_list.append(pred_item)

        # å‡†å¤‡æ€§èƒ½æŒ‡æ ‡
        metrics_dict = {}
        for model_name, metrics in results['metrics'].items():
            metrics_dict[model_name] = {
                'mae': float(metrics['mae']),
                'rmse': float(metrics['rmse']),
                'r2': float(metrics['r2']),
                'mape': float(metrics['mape']),
                'direction_accuracy': float(metrics['direction_accuracy'])
            }

        print(f"\nâœ… æ‰€æœ‰æ¨¡å‹é¢„æµ‹å®Œæˆï¼")
        print(f"   è¿”å› {len(predictions_list)} æ¡é¢„æµ‹ç»“æœ")
        print(f"   åŒ…å« {len(metrics_dict)} ä¸ªæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡")

        return jsonify({
            'success': True,
            'predictions': predictions_list,
            'metrics': metrics_dict,
            'model_names': list(results['predictions'].keys())
        })

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'é¢„æµ‹å¤±è´¥: {str(e)}'}), 500

@app.route('/api/bidding/optimize', methods=['POST'])
def optimize_bidding():
    """
    è¿è¡ŒæŠ•æ ‡ä¼˜åŒ–
    """
    try:
        print("\n" + "="*60)
        print("ğŸ¯ æ”¶åˆ°æŠ•æ ‡ä¼˜åŒ–è¯·æ±‚")
        print("="*60)

        # è¿è¡ŒæŠ•æ ‡ä¼˜åŒ–
        results = run_bidding_optimization()

        if not results.get('success', False):
            return jsonify(results), 500

        print("\nâœ… æŠ•æ ‡ä¼˜åŒ–æˆåŠŸå®Œæˆ")
        return jsonify(results)

    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"\nâŒ æŠ•æ ‡ä¼˜åŒ–å¤±è´¥: {str(e)}")
        print(error_trace)
        return jsonify({
            'success': False,
            'error': f'æŠ•æ ‡ä¼˜åŒ–å¤±è´¥: {str(e)}',
            'traceback': error_trace
        }), 500

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸš€ ç”µåŠ›å¸‚åœºé¢„æµ‹ç³»ç»Ÿå¯åŠ¨ä¸­...")
    print("=" * 60)
    print(f"ğŸ“ API åœ°å€: http://localhost:5000")
    print(f"ğŸ“Š å¥åº·æ£€æŸ¥: http://localhost:5000/health")
    print("=" * 60)
    app.run(host='0.0.0.0', port=5000, debug=True)

